{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text Classification\n",
    "After experimentation, in this notebook we perform text classification to an unknown testing dataset,\n",
    "containing almost 50K documents.\n",
    "The method consists of Lemmatization using word position tags, and baggings smv classifiers. \n",
    "We used bagging classifiers in order to enable svm to run into multiple cores, with the view to reduce the execution time \n",
    "of the plain svm.\n",
    "\n",
    "### In the end it achieves accuracy up to 0.9652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "train_path = \"files/data/train.csv\"\n",
    "predicitions_path = 'files/data/predictions.csv'\n",
    "test_path = \"files/data/test_without_labels.csv\"\n",
    "x_vectors_path = \"files/serialized/vectors\"\n",
    "test_vectors_path = \"files/serialized/tets_vectors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pickle Store and load\n",
    "Used in order to store and load the produced vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pickle_store(obj, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def pickle_load(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluation(clf, clf_name, X, y, k=5):\n",
    "    starting_tm = time.time()\n",
    "    clf_precision = 0\n",
    "    clf_recall = 0\n",
    "    clf_f1 = 0\n",
    "    clf_accuracy = 0\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        clf_precision += metrics.precision_score(y_test, predictions, average='micro')\n",
    "        clf_recall += metrics.recall_score(y_test, predictions, average='micro')\n",
    "        clf_f1 += metrics.f1_score(y_test, predictions, average='micro')\n",
    "        clf_accuracy += metrics.accuracy_score(y_test, predictions)\n",
    "    \n",
    "     # compute the average of each value\n",
    "    precision_score = clf_precision/k\n",
    "    recall_score = clf_recall/k\n",
    "    f1_score = clf_f1/k\n",
    "    accuracy_score = clf_accuracy/k\n",
    "    \n",
    "    print(clf_name + \"\\nPrecision: \" + str(precision_score)\n",
    "          + \"\\nRecall: \" + str(recall_score)\n",
    "          + \"\\nF1-Measure: \" + str(f1_score) \n",
    "          + \"\\nAccuracy: \" + str(accuracy_score)\n",
    "          + \"\\nExecution time: \" + str(time.time() - starting_tm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Data from Local store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227464</td>\n",
       "      <td>Netflix is coming to cable boxes, and Amazon i...</td>\n",
       "      <td>if you subscribe to one of three rinky-dink (...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244074</td>\n",
       "      <td>Pharrell, Iranian President React to Tehran 'H...</td>\n",
       "      <td>pharrell, iranian president react to tehran '...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60707</td>\n",
       "      <td>Wildlife service seeks comments</td>\n",
       "      <td>the u.s. fish and wildlife service has reopen...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27883</td>\n",
       "      <td>Facebook teams up with Storyful to launch 'FB ...</td>\n",
       "      <td>the very nature of social media means it is o...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169596</td>\n",
       "      <td>Caesars plans US$880 mln New York casino</td>\n",
       "      <td>caesars plans us$880 mln new york casino jul ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              Title  \\\n",
       "0  227464  Netflix is coming to cable boxes, and Amazon i...   \n",
       "1  244074  Pharrell, Iranian President React to Tehran 'H...   \n",
       "2   60707                    Wildlife service seeks comments   \n",
       "3   27883  Facebook teams up with Storyful to launch 'FB ...   \n",
       "4  169596           Caesars plans US$880 mln New York casino   \n",
       "\n",
       "                                             Content          Label  \n",
       "0   if you subscribe to one of three rinky-dink (...  Entertainment  \n",
       "1   pharrell, iranian president react to tehran '...  Entertainment  \n",
       "2   the u.s. fish and wildlife service has reopen...     Technology  \n",
       "3   the very nature of social media means it is o...     Technology  \n",
       "4   caesars plans us$880 mln new york casino jul ...       Business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "X = (train['Title']+ \" \")*5 + train['Content']\n",
    "y = le.fit_transform(train['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-process using Lemmatization\n",
    "\n",
    "Applying Lemmatization using position tags. We use position tags in order to enable lemmatization, \n",
    "not only to nouns but also to all other parts of speech. Also removing stopwords, punctuations and non alpha characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:    \n",
    "      return None\n",
    "\n",
    "my_stopwords = ENGLISH_STOP_WORDS.union(stopwords.words('english'))\\\n",
    "    .union(['include', 'way', 'work', 'look', 'add', 'time', 'year', 'month', 'day', 'help', 'think', 'tell', 'new', 'said', 'say','need', 'come', 'good', 'set', 'want', 'people', 'use', 'day', 'week', 'know'])\n",
    "\n",
    "my_stopwords_lemma = set()\n",
    "for word, nltk_tag in nltk.pos_tag(my_stopwords):\n",
    "    tag = nltk2wn_tag(nltk_tag)\n",
    "    if tag is not None:\n",
    "        my_stopwords_lemma.add(lmtzr.lemmatize(word, tag))\n",
    "    else:\n",
    "        my_stopwords_lemma.add(word)\n",
    "        \n",
    "\n",
    "def documents_preprocess(documents):\n",
    "    new_documents = []\n",
    "    starting_tm = time.time()\n",
    "    for doc in documents:\n",
    "        clean_doc = []\n",
    "        doc_tokens = simple_preprocess(doc, deacc=True)\n",
    "        for word, nltk_tag in  nltk.pos_tag(doc_tokens):\n",
    "            tag = nltk2wn_tag(nltk_tag)\n",
    "            if tag is not None:\n",
    "                lemma = lmtzr.lemmatize(word, tag)\n",
    "                if lemma not in my_stopwords_lemma:\n",
    "                    clean_doc.append(lemma)\n",
    "            else:\n",
    "                if word not in my_stopwords:\n",
    "                    clean_doc.append(word)\n",
    "        new_documents.append(clean_doc)\n",
    "    \n",
    "    print(\"Text Preprocessing took: \" + str(time.time() - starting_tm))\n",
    "    return new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing took: 1380.5122604370117\n"
     ]
    }
   ],
   "source": [
    "X = documents_preprocess(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-process and Vectorization\n",
    "We use the hashing trick and then tf-idf transformer in order to convert words frequencies into TF-IDF values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization time: 5.9197611808776855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=100000, lowercase=False, tokenizer=lambda x: x)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "starting_tm = time.time()\n",
    "vtrain = vectorizer.fit_transform(X)\n",
    "vtrain = tfidf_transformer.fit_transform(vtrain)\n",
    "print(\"Vectorization time: \" + str((time.time() - starting_tm)))\n",
    "\n",
    "pickle_store(vtrain, x_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vtrain = pickle_load(x_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baggings SVM\n",
      "Precision: 0.9648642604767655\n",
      "Recall: 0.9648642604767655\n",
      "F1-Measure: 0.9648642604767655\n",
      "Accuracy: 0.9648642604767655\n",
      "Execution time: 1137.402369260788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "n = 8\n",
    "svm_clf = svm.SVC(gamma=0.1, C=10, kernel='rbf')\n",
    "bagging_clf = BaggingClassifier(svm_clf, n_estimators=n, max_samples=1/n, n_jobs=n)\n",
    "\n",
    "evaluation(bagging_clf, \"Baggings SVM\",  vtrain, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training and Predicting\n",
    "First load the testing set, clean it and vectorize it, and then fit the classifier to the \n",
    "training set and predict the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing took: 590.4956665039062\n",
      "Vectorization time: 2.601217031478882\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(test_path)\n",
    "X_test = (test['Title']+ \" \")*5 + test['Content']\n",
    "\n",
    "X_test = documents_preprocess(X_test)\n",
    "starting_tm = time.time()\n",
    "vtest = vectorizer.transform(X_test)\n",
    "vtest = tfidf_transformer.transform(vtest)\n",
    "print(\"Vectorization time: \" + str((time.time() - starting_tm)))\n",
    "\n",
    "pickle_store(vtest, test_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vtest = pickle_load(test_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "n = 5\n",
    "svm_clf = svm.SVC(gamma=0.1, C=10, kernel='rbf')\n",
    "bagging_clf = BaggingClassifier(svm_clf, n_estimators=n, max_samples=1/n, n_jobs=n)\n",
    "\n",
    "bagging_clf.fit(vtrain, y)\n",
    "predictions = bagging_clf.predict(vtest)\n",
    "\n",
    "predictions = le.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving predictions as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(test_path)\n",
    "predictions_df = pd.DataFrame(data={'Id': list(test['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(predicitions_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}