{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "pycharm-80ab35c8",
   "language": "python",
   "display_name": "PyCharm (DSIT-BigDataAnalytics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "DL_Classification.ipynb",
   "provenance": []
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    },
    "id": "mNFQblGx0x_g",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "outputId": "1d7f7bcd-56c6-4345-c273-d362057bc2c8"
   },
   "source": [
    "# Text Classification\n",
    "In this notebook we perform text classification to an unknown testing dataset,\n",
    "containing almost 50K documents using Deep Learning approaches.\n",
    "\n",
    "In more details in this notebook we transform the texts into word Îµmbeddings vectors and then fit them\n",
    "to a CNN. The embeddings vectors are produced using keras Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import keras as K\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_Vq7Asg70x_r",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_path = \"files/data/train.csv\"\n",
    "predicitions_path = 'files/data/predictions.csv'\n",
    "test_path = \"files/data/test_without_labels.csv\""
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jugFzxSD1N5S",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "outputId": "0d5be95a-5f86-4551-f0a7-4296b50d5f87",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Colab Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "train_path = '/content/drive/My Drive/Colab Notebooks/BDA/text_classification/train.csv'\n",
    "test_path = '/content/drive/My Drive/Colab Notebooks/BDA/text_classification/test_without_labels.csv'\n",
    "predicitions_path = '/content/drive/My Drive/Colab Notebooks/BDA/text_classification/predictions.csv'\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    },
    "id": "28fqRIc20x_x",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "f50b457e-97e1-4b89-f140-fa647e19aa5f"
   },
   "source": [
    "## Loading Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "X = (train['Title']+ \" \")*5 + train['Content']\n",
    "X = X.values.tolist()\n",
    "y = train['Label']\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc_y = enc.fit_transform(y.values.reshape((-1,1)))\n",
    "print(\"No Documents: \" + str(len(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    },
    "id": "l__8lUuX0x_2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Pre-process using Lemmatization\n",
    "\n",
    "Applying Lemmatization using position tags. We use position tags in order to enable lemmatization, \n",
    "not only to nouns but also to all other parts of speech. Also removing stopwords, punctuations and non alpha characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:    \n",
    "      return None\n",
    "\n",
    "my_stopwords = ENGLISH_STOP_WORDS.union(stopwords.words('english'))\\\n",
    "    .union(['include', 'way', 'work', 'look', 'add', 'time', 'year', 'month', 'day', 'help', 'think', 'tell', 'new', 'said', 'say','need', 'come', 'good', 'set', 'want', 'people', 'use', 'day', 'week', 'know'])\n",
    "\n",
    "my_stopwords_lemma = set()\n",
    "for word, nltk_tag in nltk.pos_tag(my_stopwords):\n",
    "    tag = nltk2wn_tag(nltk_tag)\n",
    "    if tag is not None:\n",
    "        my_stopwords_lemma.add(lmtzr.lemmatize(word, tag))\n",
    "    else:\n",
    "        my_stopwords_lemma.add(word)\n",
    "        \n",
    "\n",
    "def documents_preprocess(documents):\n",
    "    new_documents = []\n",
    "    starting_tm = time.time()\n",
    "    for doc in documents:\n",
    "        clean_doc = []\n",
    "        doc_tokens = simple_preprocess(doc, deacc=True)\n",
    "        for word, nltk_tag in  nltk.pos_tag(doc_tokens):\n",
    "            tag = nltk2wn_tag(nltk_tag)\n",
    "            if tag is not None:\n",
    "                lemma = lmtzr.lemmatize(word, tag)\n",
    "                if lemma not in my_stopwords_lemma:\n",
    "                    clean_doc.append(lemma)\n",
    "            else:\n",
    "                if word not in my_stopwords:\n",
    "                    clean_doc.append(word)\n",
    "        new_documents.append(clean_doc)\n",
    "    \n",
    "    print(\"Text Preprocessing took: \" + str(time.time() - starting_tm))\n",
    "    return new_documents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LD4kc_TH2OS-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "1215a02a-100c-4d38-a60c-02b6d32dacee"
   },
   "source": [
    "clean_X = documents_preprocess(X)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Text Preprocessing took: 200.06799244880676\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    },
    "id": "37gfHyZ00x_7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "36fcfbae-b204-493d-bb29-ba6f8c70e1b8"
   },
   "source": [
    "## Embedding Configuration\n",
    "Computing the size of the vocabulary and the max length of the documents. Then, pad all the documents,\n",
    "in order to reach the max lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words_set = set()\n",
    "sentence_max_words = max([len(doc) for doc in clean_X])\n",
    "for doc in clean_X: words_set.update(doc)\n",
    "\n",
    "vocab_length = len(words_set)\n",
    "print(\"Dictionary size: \" + str(vocab_length) + \"\\nMax words per sentence: \" + str(sentence_max_words))\n",
    "\n",
    "embedded_doc = [one_hot(\" \".join(doc), vocab_length) for doc in clean_X]\n",
    "padded_doc = pad_sequences(embedded_doc, sentence_max_words, padding='post')\n",
    "padded_doc.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    },
    "id": "68eM7nDy0yAD",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "outputId": "73e39b0d-dade-4d58-b35d-348068fcb20b"
   },
   "source": [
    "## E+CNN Architecture\n",
    "The model consists of the following layers:\n",
    "- An Embedding Layer, which will transform the input documents into embedding vectors\n",
    "- Two 1D Convolution layers followed by max pooling layers\n",
    "- Two Dense layers and an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_dim = 80\n",
    "es = K.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=3)\n",
    "\n",
    "\n",
    "def create_model(summary=False):\n",
    "  model = K.models.Sequential([\n",
    "      K.layers.Embedding(vocab_length+1, embedding_dim, input_length=sentence_max_words),\n",
    "      K.layers.Conv1D(64, 16, padding='valid', activation='relu', strides=2, name=\"cov1\"),\n",
    "      K.layers.MaxPool1D(), \n",
    "      K.layers.Conv1D(64, 16, padding='same', activation='relu', name=\"cov2\"),\n",
    "      K.layers.GlobalMaxPooling1D(),\n",
    "      K.layers.Dropout(0.2),\n",
    "      K.layers.Dense(32, activation='relu'),\n",
    "      K.layers.Dense(16, activation='relu'),\n",
    "      K.layers.Dense(4, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer=K.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  if summary: \n",
    "    model.summary()\n",
    "  return model\n",
    "\n",
    "create_model(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JFyG00Y6D6a",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation\n",
    "Perform evaluation using k-Fold Cross Validation. In each iteration the model is constructed\n",
    " from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "starting_tm = time.time()\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "accuracy = 0\n",
    "\n",
    "k = 5\n",
    "epochs = 5\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "for train_index, test_index in skf.split(padded_doc, y):\n",
    "    \n",
    "    X_train, X_test = padded_doc[train_index], padded_doc[test_index]\n",
    "    y_train, y_test = enc_y[train_index], enc_y[test_index]\n",
    "    \n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=256)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    precision += metrics.precision_score(y_test, predictions, average='micro')\n",
    "    recall += metrics.recall_score(y_test, predictions, average='micro')\n",
    "    f1 += metrics.f1_score(y_test, predictions, average='micro')\n",
    "    accuracy += metrics.accuracy_score(y_test, predictions)\n",
    "\n",
    "    print()\n",
    "\n",
    " # compute the average of each value\n",
    "precision_score = precision/k\n",
    "recall_score = recall/k\n",
    "f1_score = f1/k\n",
    "accuracy_score = accuracy/k\n",
    "\n",
    "print(\"Precision: \" + str(precision_score)\n",
    "      + \"\\nRecall: \" + str(recall_score)\n",
    "      + \"\\nF1-Measure: \" + str(f1_score) \n",
    "      + \"\\nAccuracy: \" + str(accuracy_score)\n",
    "      + \"\\nExecution time: \" + str(time.time() - starting_tm))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Z0jjlR1b0yAK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Prediction\n",
    "Load the testing dataset, pre-process it and then predict it. In the end store the results as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = pd.read_csv(test_path)\n",
    "X_test = (test['Title']+ \" \")*5 + test['Content']\n",
    "\n",
    "test_y = pd.get_dummies(test['Label']).values\n",
    "\n",
    "test_clean_X = documents_preprocess(X_test)\n",
    "\n",
    "test_embedded_doc = [one_hot(\" \".join(doc), vocab_length) for doc in test_clean_X]\n",
    "test_padded_doc = pad_sequences(test_embedded_doc, sentence_max_words, padding='post')\n",
    "print(test_padded_doc.shape)\n",
    "\n",
    "labels_dict = {[0,1,0,0]: \"Entertainment\", [1,0,0,0]: 'Business', [0,0,1,0]:'Health', [0,0,0,1]:'Technology'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "id": "_P4CXGwQ0yAN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model = create_model(True)\n",
    "model.fit(padded_doc, y,epochs=10, batch_size=64, callbacks=[es])\n",
    "\n",
    "predictions = model.predict(test_padded_doc)\n",
    "predictions = [labels_dict(prediction) for prediction in predictions]\n",
    "\n",
    "\n",
    "predictions_df = pd.DataFrame(data={'Id': list(train['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(predicitions_path, index=False)"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}