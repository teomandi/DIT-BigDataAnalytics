{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text Clasification\n",
    "\n",
    "### In this notebook we perform text classification experiments using Random Forest, SVM and Stohastic Gradient Descent Classifier.\n",
    "\n",
    "The input dataset contains over 100000 documents and the labels consists of Technology, Business,\n",
    "Health and Entertainment. In these experiments we first train our classifiers using the TF-IDF vectors evaluate them.\n",
    "Then we apply SVD in order to reduce the dimensions of the vectors and again evaluate our classifiers using the new set.\n",
    "Before each evaluation, we use grid search in order to find the best hyper-parameter of each model. Furthermore, since our\n",
    "dataset is significantly big, in vectorization we use the hashing trick in order to compute the frequencies and then we \n",
    "use the TF-IDF transformer in order to calculate the TF-IDF values. \n",
    "\n",
    "To sum up, the whole procedure consists of:  \n",
    "    1. Loading Dataset.\n",
    "    2. Text pre-processing.\n",
    "    3. TF-IDF vectorization using the hashing trick.\n",
    "    4. Grid Search for optimal hyper-parameter detection.\n",
    "    5. Evaluation of the classifiers with the TF-IDF vectors using 5-Fold Cross Validation. \n",
    "    6. Dimension reduction using SVD.\n",
    "    7. Grid search again for the new set.\n",
    "    8. Evaluation with the new set.\n",
    "---\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import preprocessing, svm, metrics\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method to implement K-Fold Cross Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def evaluation(clf, clf_name, X, y, k=5):\n",
    "    starting_tm = time.time()\n",
    "    clf_precision = 0\n",
    "    clf_recall = 0\n",
    "    clf_f1 = 0\n",
    "    clf_accuracy = 0\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        clf_precision += metrics.precision_score(y_test, predictions, average='micro')\n",
    "        clf_recall += metrics.recall_score(y_test, predictions, average='micro')\n",
    "        clf_f1 += metrics.f1_score(y_test, predictions, average='micro')\n",
    "        clf_accuracy += metrics.accuracy_score(y_test, predictions)\n",
    "    \n",
    "     # compute the average of each value\n",
    "    precision_score = clf_precision/k\n",
    "    recall_score = clf_recall/k\n",
    "    f1_score = clf_f1/k\n",
    "    accuracy_score = clf_accuracy/k\n",
    "    \n",
    "    print(clf_name + \"\\nPrecision: \" + str(precision_score)\n",
    "          + \"\\nRecall: \" + str(recall_score)\n",
    "          + \"\\nF1-Measure: \" + str(f1_score) \n",
    "          + \"\\nAccuracy: \" + str(accuracy_score)\n",
    "          + \"\\nExecution time: \" + str(time.time() - starting_tm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Search for hyper-parameter tuning\n",
    "Giving an input dataset, seek to find the optimal hyper parameters using grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def grid_evaluattion(clf, msg, X, y, tuned_parameters, scores):\n",
    "    print(msg)\n",
    "    \n",
    "    # Split the dataset in two equal parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)    \n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\\n\" % score)\n",
    "    \n",
    "        clf = GridSearchCV(clf, tuned_parameters, scoring='%s_micro' % score)\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        print(\"Best parameters set found on development set: \")\n",
    "        print(clf.best_params_)\n",
    "        print(\"\\nGrid scores on development set:\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "    \n",
    "        print(\"Detailed classification report:\\n\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\\n\")\n",
    "        \n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print() \n",
    "\n",
    "# test all the classifiers\n",
    "def hyper_parameter_tuning(train_set, labels):\n",
    "    starting_tm = time.time()\n",
    "    rf = RandomForestClassifier(n_jobs=12)\n",
    "    msg = \"Using Random Forest Classifier\"\n",
    "    tuned_parameters = [{'n_estimators': [i*10 for i in range(1, 21)]}]\n",
    "    scores = ['recall']\n",
    "    grid_evaluattion(rf, msg, train_set, labels, tuned_parameters, scores)\n",
    "    \n",
    "    svm_clf = svm.SVC()\n",
    "    msg = \"Using SMV Classifier\"\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 1e-1, 1e-2,], 'C': [10, 100, 1000]}]\n",
    "    scores = ['recall']\n",
    "    grid_evaluattion(svm_clf, msg, train_set, labels, tuned_parameters, scores)\n",
    "    \n",
    "    sgd_clf = SGDClassifier(n_jobs=12)\n",
    "    msg =\"Using Stohastic Gradient Descent\"\n",
    "    tuned_parameters = [{'loss':['hinge', 'modified_huber', 'log', 'squared_hinge'], 'max_iter':[3000]}]\n",
    "    scores = ['recall']\n",
    "    grid_evaluattion(sgd_clf, msg, train_set, labels, tuned_parameters, scores)\n",
    "    print(\"Execution time: \" + str(time.time() - starting_tm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Dataset and encoding its labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(111795, 4)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "       Id                                              Title  \\\n0  227464  Netflix is coming to cable boxes, and Amazon i...   \n1  244074  Pharrell, Iranian President React to Tehran 'H...   \n2   60707                    Wildlife service seeks comments   \n3   27883  Facebook teams up with Storyful to launch 'FB ...   \n4  169596           Caesars plans US$880 mln New York casino   \n\n                                             Content          Label  \n0   if you subscribe to one of three rinky-dink (...  Entertainment  \n1   pharrell, iranian president react to tehran '...  Entertainment  \n2   the u.s. fish and wildlife service has reopen...     Technology  \n3   the very nature of social media means it is o...     Technology  \n4   caesars plans us$880 mln new york casino jul ...       Business  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Title</th>\n      <th>Content</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>227464</td>\n      <td>Netflix is coming to cable boxes, and Amazon i...</td>\n      <td>if you subscribe to one of three rinky-dink (...</td>\n      <td>Entertainment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>244074</td>\n      <td>Pharrell, Iranian President React to Tehran 'H...</td>\n      <td>pharrell, iranian president react to tehran '...</td>\n      <td>Entertainment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60707</td>\n      <td>Wildlife service seeks comments</td>\n      <td>the u.s. fish and wildlife service has reopen...</td>\n      <td>Technology</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27883</td>\n      <td>Facebook teams up with Storyful to launch 'FB ...</td>\n      <td>the very nature of social media means it is o...</td>\n      <td>Technology</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>169596</td>\n      <td>Caesars plans US$880 mln New York casino</td>\n      <td>caesars plans us$880 mln new york casino jul ...</td>\n      <td>Business</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "train = pd.read_csv(\"files/data/train.csv\")\n",
    "X = (train['Title']+ \" \")*5 + train['Content']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(train['Label'])\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-processing using Stem Tokenizer\n",
    "For each document, remove all the non alpha characters, transform text to lower case, remove the stopwords\n",
    "and stem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "my_stopwords = set([ps.stem(w) for w in \n",
    "                  ENGLISH_STOP_WORDS\n",
    "                  .union(stopwords.words('english'))\n",
    "                  .union(['include', 'way', 'work', 'look', 'add', 'time', 'year', 'month', 'day', 'help', 'think', 'tell', 'new', 'said', 'say','need', 'come', 'good', 'set', 'want', 'people', 'use', 'day', 'week', 'know'])])\n",
    "\n",
    "def stem_tokenizer(doc):\n",
    "    clean_document = re.sub('[^a-zA-Z]+', ' ', doc.lower())\n",
    "    tokens = [ps.stem(token) for token in word_tokenize(clean_document) if len(token) > 2]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF Vectorization\n",
    "Since the corpus are too large, we apply the hashing trick using the HashingVectorizer.\n",
    "Afterwards, we calculate the TF-IDF values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Vectorization time: 668.3160946369171\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "(111795, 100000)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(tokenizer=stem_tokenizer, stop_words=my_stopwords, n_features=100000)\n",
    "\n",
    "starting_tm = time.time()\n",
    "vectors = vectorizer.fit_transform(X)\n",
    "vectors = TfidfTransformer().fit_transform(vectors)\n",
    "print(\"Vectorization time: \" + str((time.time() - starting_tm)))\n",
    "\n",
    "vectors.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classifiers Evaluation with BoW\n",
    "We examine the performance of the classifiers using the tf-idf vectors as input. In order to detect the optimal \n",
    "hyper-parameters, we apply grid search. However, since the data are too big, we use just a subset of the vectors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using Random Forest Classifier\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set: \n",
      "{'n_estimators': 140}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.826 (+/-0.012) for {'n_estimators': 10}\n",
      "0.860 (+/-0.010) for {'n_estimators': 20}\n",
      "0.869 (+/-0.015) for {'n_estimators': 30}\n",
      "0.879 (+/-0.017) for {'n_estimators': 40}\n",
      "0.880 (+/-0.011) for {'n_estimators': 50}\n",
      "0.884 (+/-0.015) for {'n_estimators': 60}\n",
      "0.890 (+/-0.008) for {'n_estimators': 70}\n",
      "0.887 (+/-0.012) for {'n_estimators': 80}\n",
      "0.889 (+/-0.015) for {'n_estimators': 90}\n",
      "0.890 (+/-0.004) for {'n_estimators': 100}\n",
      "0.891 (+/-0.006) for {'n_estimators': 110}\n",
      "0.893 (+/-0.009) for {'n_estimators': 120}\n",
      "0.892 (+/-0.008) for {'n_estimators': 130}\n",
      "0.894 (+/-0.004) for {'n_estimators': 140}\n",
      "0.893 (+/-0.010) for {'n_estimators': 150}\n",
      "0.892 (+/-0.008) for {'n_estimators': 160}\n",
      "0.893 (+/-0.012) for {'n_estimators': 170}\n",
      "0.893 (+/-0.010) for {'n_estimators': 180}\n",
      "0.894 (+/-0.012) for {'n_estimators': 190}\n",
      "0.894 (+/-0.011) for {'n_estimators': 200}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      2232\n",
      "           1       0.91      0.98      0.94      3963\n",
      "           2       0.97      0.80      0.88      1042\n",
      "           3       0.89      0.87      0.88      2763\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.91      0.88      0.89     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "Using SMV Classifier\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set: \n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.953 (+/-0.005) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.004) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.930 (+/-0.010) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.954 (+/-0.005) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.006) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.005) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.954 (+/-0.005) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.950 (+/-0.006) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.950 (+/-0.007) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      2232\n",
      "           1       0.98      0.99      0.98      3963\n",
      "           2       0.99      0.95      0.97      1042\n",
      "           3       0.94      0.95      0.95      2763\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.95      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "Using Stohastic Gradient Descent\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set: \n",
      "{'loss': 'hinge', 'max_iter': 3000}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.955 (+/-0.005) for {'loss': 'hinge', 'max_iter': 3000}\n",
      "0.955 (+/-0.004) for {'loss': 'modified_huber', 'max_iter': 3000}\n",
      "0.939 (+/-0.007) for {'loss': 'log', 'max_iter': 3000}\n",
      "0.791 (+/-0.051) for {'loss': 'squared_hinge', 'max_iter': 3000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93      2232\n",
      "           1       0.98      0.99      0.98      3963\n",
      "           2       0.98      0.96      0.97      1042\n",
      "           3       0.93      0.95      0.94      2763\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.95      0.95     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "Execution time: 2540.2746675014496\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "hyper_parameter_tuning(vectors[:20000], y[:20000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Classifiers Performance using 111795 documents.\n",
      "\n",
      "Random Forest Classifier\n",
      "Precision: 0.9356500737957869\n",
      "Recall: 0.9356500737957869\n",
      "F1-Measure: 0.9356500737957869\n",
      "Accuracy: 0.9356500737957869\n",
      "Execution time: 621.035596370697\n",
      "\n",
      "\n",
      "\n",
      "Stohastic Gradient Descent\n",
      "Precision: 0.9641397200232568\n",
      "Recall: 0.9641397200232568\n",
      "F1-Measure: 0.9641397200232568\n",
      "Accuracy: 0.9641397200232568\n",
      "Execution time: 4.2480950355529785\n",
      "\n",
      "\n",
      "\n",
      "SVM Classifier\n",
      "Precision: 0.975481908851022\n",
      "Recall: 0.975481908851022\n",
      "F1-Measure: 0.975481908851022\n",
      "Accuracy: 0.975481908851022\n",
      "Execution time: 14560.414593935013\n",
      "\n",
      "\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Classifiers Performance using \" + str(vectors.shape[0]) + \" documents.\\n\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=140, n_jobs=12)\n",
    "svm_clf =svm.SVC(gamma=0.1, C=10, kernel='rbf')\n",
    "sgd_clf = SGDClassifier(loss='hinge', max_iter=3000, n_jobs=12)\n",
    "\n",
    "clfs = [(rf, \"Random Forest Classifier\"), (sgd_clf, \"Stohastic Gradient Descent\"), (svm_clf, \"SVM Classifier\")]\n",
    "for clf, clf_name in clfs:\n",
    "    evaluation(clf, clf_name, vectors, y)\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dimension Reduction with SVD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Produced dataset's shape is (111795, 200)\n",
      "SVD time: 31.66509222984314\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "components = 200\n",
    "lsi_model = TruncatedSVD(n_components=components)\n",
    "starting_tm = time.time()\n",
    "lsi_X = lsi_model.fit_transform(vectors, y)\n",
    "print(\"Produced dataset's shape is \" + str(lsi_X.shape))\n",
    "print(\"SVD time: \" + str((time.time() - starting_tm)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classifier Evaluation after SVD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using Random Forest Classifier\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set: \n",
      "{'n_estimators': 200}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.911 (+/-0.007) for {'n_estimators': 10}\n",
      "0.921 (+/-0.007) for {'n_estimators': 20}\n",
      "0.922 (+/-0.008) for {'n_estimators': 30}\n",
      "0.924 (+/-0.008) for {'n_estimators': 40}\n",
      "0.924 (+/-0.008) for {'n_estimators': 50}\n",
      "0.925 (+/-0.008) for {'n_estimators': 60}\n",
      "0.926 (+/-0.008) for {'n_estimators': 70}\n",
      "0.926 (+/-0.004) for {'n_estimators': 80}\n",
      "0.926 (+/-0.009) for {'n_estimators': 90}\n",
      "0.927 (+/-0.011) for {'n_estimators': 100}\n",
      "0.925 (+/-0.006) for {'n_estimators': 110}\n",
      "0.927 (+/-0.007) for {'n_estimators': 120}\n",
      "0.926 (+/-0.011) for {'n_estimators': 130}\n",
      "0.926 (+/-0.011) for {'n_estimators': 140}\n",
      "0.927 (+/-0.007) for {'n_estimators': 150}\n",
      "0.928 (+/-0.008) for {'n_estimators': 160}\n",
      "0.928 (+/-0.008) for {'n_estimators': 170}\n",
      "0.927 (+/-0.007) for {'n_estimators': 180}\n",
      "0.927 (+/-0.006) for {'n_estimators': 190}\n",
      "0.928 (+/-0.008) for {'n_estimators': 200}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      2232\n",
      "           1       0.96      0.98      0.97      3963\n",
      "           2       0.94      0.89      0.92      1042\n",
      "           3       0.91      0.93      0.92      2763\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.92      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "\n",
      "Using SMV Classifier\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set: \n",
      "{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.940 (+/-0.007) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.928 (+/-0.009) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.896 (+/-0.015) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.935 (+/-0.007) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.006) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.927 (+/-0.009) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.926 (+/-0.007) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.932 (+/-0.004) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.007) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91      2232\n",
      "           1       0.97      0.98      0.97      3963\n",
      "           2       0.95      0.93      0.94      1042\n",
      "           3       0.93      0.94      0.93      2763\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n",
      "\n",
      "Using Stohastic Gradient Descent\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set: \n",
      "{'loss': 'modified_huber', 'max_iter': 3000}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.924 (+/-0.009) for {'loss': 'hinge', 'max_iter': 3000}\n",
      "0.926 (+/-0.014) for {'loss': 'modified_huber', 'max_iter': 3000}\n",
      "0.911 (+/-0.007) for {'loss': 'log', 'max_iter': 3000}\n",
      "0.908 (+/-0.015) for {'loss': 'squared_hinge', 'max_iter': 3000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      2232\n",
      "           1       0.97      0.97      0.97      3963\n",
      "           2       0.95      0.89      0.92      1042\n",
      "           3       0.92      0.91      0.92      2763\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.92      0.92     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "\n",
      "Execution time: 409.72021985054016\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/gmandi/miniconda3/envs/AI/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "hyper_parameter_tuning(lsi_X[:20000], y[:20000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Classifiers Performance using 111795 documents.\n",
      "\n",
      "Random Forest Classifier\n",
      "Precision: 0.95464018963281\n",
      "Recall: 0.95464018963281\n",
      "F1-Measure: 0.95464018963281\n",
      "Accuracy: 0.95464018963281\n",
      "Execution time: 255.4513065814972\n",
      "\n",
      "\n",
      "\n",
      "SVM Classifier\n",
      "Precision: 0.9590858267364373\n",
      "Recall: 0.9590858267364373\n",
      "F1-Measure: 0.9590858267364373\n",
      "Accuracy: 0.9590858267364373\n",
      "Execution time: 1595.4284834861755\n",
      "\n",
      "\n",
      "\n",
      "Stohastic Gradient Descent\n",
      "Precision: 0.92914709960195\n",
      "Recall: 0.92914709960195\n",
      "F1-Measure: 0.92914709960195\n",
      "Accuracy: 0.92914709960195\n",
      "Execution time: 3.627742052078247\n",
      "\n",
      "\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Classifiers Performance using \" + str(lsi_X.shape[0]) + \" documents.\\n\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, n_jobs=12)\n",
    "svm_clf =svm.SVC(gamma=1, C=10, kernel='rbf')\n",
    "sgd_clf = SGDClassifier(loss='modified_huber', max_iter=3000, n_jobs=12)\n",
    "\n",
    "clfs = [(rf, \"Random Forest Classifier\"), (svm_clf, \"SVM Classifier\"), (sgd_clf, \"Stohastic Gradient Descent\")]\n",
    "for clf, clf_name in clfs:\n",
    "    evaluation(clf, clf_name, lsi_X, y)\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}