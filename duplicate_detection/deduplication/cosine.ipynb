{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "cosine.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exact Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "colab_type": "code",
        "id": "9YhNx_JyF9Si",
        "outputId": "7cb4a4c1-7fe8-41c0-8beb-0368480672ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import math \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "from gensim.utils import simple_preprocess\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "colab_type": "code",
        "id": "cS2kaywqGA2n",
        "outputId": "cdf413cf-747b-4ab7-810c-c642a7662e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "74IOafpvGBXU",
        "outputId": "b3e38f76-36a3-49d1-9f31-556e3c2eb096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lEsSvUwjF9TO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def pickle_store(obj, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(obj, file)\n",
        "\n",
        "def pickle_load(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "        \n",
        "# angular similarity according to wikipidia (not used)\n",
        "# http://en.wikipedia.org/wiki/Cosine_similarity\n",
        "def angular_similarity(a,b):\n",
        "    dot_prod = np.dot(a,b)\n",
        "    sum_a = sum(a**2) **.5\n",
        "    sum_b = sum(b**2) **.5\n",
        "    cosine = dot_prod/sum_a/sum_b # cosine similarity\n",
        "    theta = math.acos(cosine)\n",
        "    return 1.0-(theta/math.pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MHzOjQN8F9Ti"
      },
      "source": [
        "## Document preprocesser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ggLNyzdsF9Ty"
      },
      "outputs": [],
      "source": [
        "lmtzr = WordNetLemmatizer()\n",
        "\n",
        "def nltk2wn_tag(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif nltk_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif nltk_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:    \n",
        "      return None\n",
        "\n",
        "my_stopwords = ENGLISH_STOP_WORDS.union(stopwords.words('english'))\\\n",
        "    .union(['new', 'said', 'say','need', 'come', 'good', 'set', 'want', 'people', 'use', 'day', 'week', 'know'])\n",
        "\n",
        "my_stopwords_lemma = set()\n",
        "for word, nltk_tag in nltk.pos_tag(my_stopwords):\n",
        "    tag = nltk2wn_tag(nltk_tag)\n",
        "    if tag is not None:\n",
        "        my_stopwords_lemma.add(lmtzr.lemmatize(word, tag))\n",
        "    else:\n",
        "        my_stopwords_lemma.add(word)\n",
        "        \n",
        "\n",
        "def documents_preprocess(documents):\n",
        "    new_documents = []\n",
        "    starting_tm = time.time()\n",
        "    for doc in documents:\n",
        "        clean_doc = []\n",
        "        doc_tokens = simple_preprocess(doc, deacc=True)\n",
        "        for word, nltk_tag in  nltk.pos_tag(doc_tokens):\n",
        "            tag = nltk2wn_tag(nltk_tag)\n",
        "            if tag is not None:\n",
        "                lemma = lmtzr.lemmatize(word, tag)\n",
        "                if lemma not in my_stopwords_lemma:\n",
        "                    clean_doc.append(lemma)\n",
        "            else:\n",
        "                if word not in my_stopwords:\n",
        "                    clean_doc.append(word)\n",
        "        new_documents.append(' '.join(clean_doc))\n",
        "    \n",
        "    print(\"Text Preprocessing took: \" + str(time.time() - starting_tm))\n",
        "    return new_documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Aqxn3RFF9UF"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "AIXwXv0FF9UP",
        "outputId": "c9ea1ce5-03d9-4f52-8af4-fc0ea438c1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  531990 Test:  5374\n"
          ]
        }
      ],
      "source": [
        "# train = pd.read_csv(\"data/corpusTrain.csv\")\n",
        "# test = pd.read_csv(\"data/corpusTest.csv\")\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/corpusTrain.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/corpusTest.csv\")\n",
        "\n",
        "# train=train[:10000]\n",
        "# test=test[:1000]\n",
        "\n",
        "print(\"Train: \", len(train), \"Test: \", len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnxLBaHHF9Ul"
      },
      "source": [
        "## Clean the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "colab_type": "code",
        "id": "QldMZhPxF9Um",
        "outputId": "2310225a-6c94-47f0-896e-3207d1cbdfd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Preprocessing took: 338.6784873008728\n",
            "Text Preprocessing took: 3.4820220470428467\n",
            "Clean:  342.16618371009827\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "clean_train = documents_preprocess(train['Content'])\n",
        "clean_test = documents_preprocess(test['Content'])\n",
        "\n",
        "print(\"Clean: \", time.time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x1xG7E_SF9U1"
      },
      "outputs": [],
      "source": [
        "# pickle_store(clean_train, \"vars/full_clean_train\")\n",
        "# pickle_store(clean_test, \"vars/full_clean_test\")\n",
        "\n",
        "print(\"done\")\n",
        "\n",
        "clean_train = pickle_load(\"vars/full_clean_train\")\n",
        "clean_test = pickle_load(\"vars/full_clean_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aWFVT5QXF9VB"
      },
      "source": [
        "## Vectorize \n",
        "- hashingVectorizer\n",
        "- CountVectorizer (Current in use)\n",
        "- TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "17Bf6TvjF9VS",
        "outputId": "e7f826d2-fa45-4088-d653-bc646ba395c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorization Time:  3.8326215744018555\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# vectorizer = HashingVectorizer(stop_words=my_stopwords, n_features=8000)\n",
        "# vectorizer = TfidfVectorizer(stop_words=my_stopwords, max_features=50000)\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "vtrain = vectorizer.fit_transform(clean_train)\n",
        "vtest = vectorizer.transform(clean_test)\n",
        "\n",
        "print(\"Vectorization Time: \", time.time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aPRdnyVPF9Vl"
      },
      "outputs": [],
      "source": [
        "pickle_store(vtrain, \"vars/vtrain_countFull\")\n",
        "pickle_store(vtest, \"vars/vtest_countFull\")\n",
        "\n",
        "# vtrain = pickle_load(\"vars/vtrain_tf2000\")\n",
        "# vtest = pickle_load(\"vars/vtest_tf2000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "LT8vrqD6F9V0",
        "outputId": "44c727c2-2361-4816-8237-00cf619dc1b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5374, 70052)\n",
            "(531990, 70052)\n"
          ]
        }
      ],
      "source": [
        "print(vtest.shape)\n",
        "print(vtrain.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xyfE-77EF9Wj"
      },
      "source": [
        "## Run Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Bc13XGFTF9Wu",
        "outputId": "c495c032-9e3b-40fd-b120-0bc8255ac493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "queue:  430.126925945282\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "duplicates = 0 \n",
        "\n",
        "for vt in vtest:\n",
        "    \n",
        "    results = cosine_similarity(vt, vtrain)\n",
        "    duplicates += (results > 0.8).sum()\n",
        "\n",
        "q_time = time.time()- start\n",
        "\n",
        "print(\"queue: \", q_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "2ZQSsNu8F9Wy",
        "outputId": "0cc3f261-ea22-42ce-82a2-c67cc6e0d1cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27087"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NiKQY95TF9XC"
      },
      "source": [
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4O0GXYeDF9XE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVD (not used in final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pL26iJYtF9WJ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "svd = TruncatedSVD(n_components=100)\n",
        "sv_train = svd.fit_transform(vtrain)\n",
        "sv_test= svd.transform(vtest)\n",
        "\n",
        "print(\"SVD: \", time.time()-start)\n",
        "\n",
        "pickle_store(sv_train, \"vars/sv_train_100\")\n",
        "pickle_store(sv_test, \"vars/sv_test100\")\n",
        "\n",
        "# sv_train = pickle_load(\"vars/sv_train_100\")\n",
        "# sv_test = pickle_load(\"vars/sv_test100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizer (not used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tJ0sDhUqF9XR"
      },
      "outputs": [],
      "source": [
        "\n",
        "minx = -1 \n",
        "maxx = 1\n",
        "\n",
        "normalize = lambda x : (x - minx)/(maxx-minx)\n",
        "vfunc = np.vectorize(normalize)\n",
        "results2 = vfunc(results)\n",
        "\n",
        "(results2 >= 0.8).sum()"
      ]
    }
  ]
}