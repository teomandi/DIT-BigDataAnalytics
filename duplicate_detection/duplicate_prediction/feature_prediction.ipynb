{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicates Predictions \n",
    "## Based on Features Engineerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing, svm, metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pickle_store(obj, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def pickle_load(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(clf, clf_name, X, y, k=5):\n",
    "    starting_tm = time.time()\n",
    "    clf_precision = 0\n",
    "    clf_recall = 0\n",
    "    clf_f1 = 0\n",
    "    clf_accuracy = 0\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        clf_precision += metrics.precision_score(y_test, predictions, average='micro')\n",
    "        clf_recall += metrics.recall_score(y_test, predictions, average='micro')\n",
    "        clf_f1 += metrics.f1_score(y_test, predictions, average='micro')\n",
    "        clf_accuracy += metrics.accuracy_score(y_test, predictions)\n",
    "    \n",
    "     # compute the average of each value\n",
    "    precision_score = clf_precision/k\n",
    "    recall_score = clf_recall/k\n",
    "    f1_score = clf_f1/k\n",
    "    accuracy_score = clf_accuracy/k\n",
    "    \n",
    "    print(clf_name + \"\\nPrecision: \" + str(precision_score)\n",
    "          + \"\\nRecall: \" + str(recall_score)\n",
    "          + \"\\nF1-Measure: \" + str(f1_score) \n",
    "          + \"\\nAccuracy: \" + str(accuracy_score)\n",
    "          + \"\\nExecution time: \" + str(time.time() - starting_tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def grid_evaluattion(clf, msg, X, y, tuned_parameters, scores):\n",
    "    print(msg)\n",
    "    \n",
    "    # Split the dataset in two equal parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)    \n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\\n\" % score)\n",
    "    \n",
    "        clf = GridSearchCV(clf, tuned_parameters, scoring=score)\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        print(\"Best parameters set found on development set: \")\n",
    "        print(clf.best_params_)\n",
    "        print(\"\\nGrid scores on development set:\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "    \n",
    "        print(\"Detailed classification report:\\n\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\\n\")\n",
    "        \n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "283004\n"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "data = data.fillna('')\n",
    "y = data[\"IsDuplicate\"]\n",
    "print(len(data[\"IsDuplicate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction\n",
    "\n",
    "## Basic Fetures\n",
    "- Length of question1\n",
    "- Length of question2\n",
    "- Difference between the two lengths\n",
    "- Character length of question1 without spaces\n",
    "- Character length of question2 without spaces\n",
    "- Number of words in question1\n",
    "- Number of words in question2\n",
    "- Number of common words in question1 and question2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['len_q1'] = data[\"Question1\"].apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data[\"Question2\"].apply(lambda x: len(str(x)))\n",
    "data['diff_len'] = data.len_q1 - data.len_q2\n",
    "\n",
    "# character length based features\n",
    "data['len_char_q1'] = data[\"Question1\"].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data['len_char_q2'] = data[\"Question2\"].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "\n",
    "# word length based features\n",
    "data['len_word_q1'] = data[\"Question1\"].apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data[\"Question2\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# common words in the two questions\n",
    "data['common_words'] = data.apply(lambda x: len(set(str(x[[\"Question1\"]]).lower().split()).intersection(set(str(x['Question2']).lower().split()))), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_1 = ['len_q1', 'len_q2', 'diff_len',\n",
    "    'len_char_q1', 'len_char_q2', \n",
    "    'len_word_q1', 'len_word_q2', \n",
    "    'common_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Features\n",
    "- QRatio\n",
    "- WRatio\n",
    "- Partial ratio\n",
    "- Partial token set ratio\n",
    "- Partial token sort ratio\n",
    "- Token set ratio\n",
    "- Token sort ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "data['fuzz_partial_token_set_ratio'] = data.apply(lambda x:fuzz.partial_token_set_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['Question1']), str(x['Question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_2 = ['fuzz_qratio', 'fuzz_WRatio',\n",
    "    'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio', \n",
    "    'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', \n",
    "    'fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping with Word2vec embeddings\n",
    "### Features:\n",
    "- Cosine distance between vectors of question1 and question2\n",
    "- Manhattan distance between vectors of question1 and question2\n",
    "- Jaccard similarity between vectors of question1 and question2\n",
    "- Canberra distance between vectors of question1 and question2\n",
    "- Euclidean distance between vectors of question1 and question2\n",
    "- Minkowski distance between vectors of question1 and question2\n",
    "- Braycurtis distance between vectors of question1 and question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to /home/teomandi/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/teomandi/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#adds the vectors for all words in a sentence that are available \n",
    "#in the Google news vectors and gives a normalized vector at the end\n",
    "def sentence2vec(s, model): \n",
    "    M = []\n",
    "    words = word_tokenize(str(s).lower())\n",
    "    for word in words:\n",
    "        #It shouldn't be a stopword\n",
    "        if word not in stop_words:\n",
    "            #nor contain numbers\n",
    "            if word.isalpha():\n",
    "                #and be part of word2vec\n",
    "                if word in model:\n",
    "                    M.append(model[word])\n",
    "    M = np.array(M)  # <---\n",
    "    if len(M) > 0:\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    else:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_q1 = np.array([sentence2vec(q, model) for q in data[\"Question1\"]])\n",
    "w2v_q2 = np.array([sentence2vec(q, model) for q in data[\"Question2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "data['cosine_distance'] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['cityblock_distance'] = [cityblock(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['jaccard_distance'] = [jaccard(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['canberra_distance'] = [canberra(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['euclidean_distance'] = [euclidean(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['minkowski_distance'] = [minkowski(x,y,3) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data['braycurtis_distance'] = [braycurtis(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_3 = ['cosine_distance', 'cityblock_distance', \n",
    "         'jaccard_distance', 'canberra_distance', \n",
    "         'euclidean_distance', 'minkowski_distance',\n",
    "         'braycurtis_distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data[ feats_1 + feats_2 + feats_3 ]\n",
    "d = d.fillna(d.mean())\n",
    "train = d.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_store(train, \"vars/trainf1f2f3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Paramaters Look up\n",
    "\n",
    "- ## Stohastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Using Stohastic Gradient Descent\n# Tuning hyper-parameters for accuracy\n\nBest parameters set found on development set: \n{'loss': 'modified_huber', 'max_iter': 3000}\n\nGrid scores on development set:\n0.635 (+/-0.043) for {'loss': 'hinge', 'max_iter': 3000}\n0.656 (+/-0.043) for {'loss': 'modified_huber', 'max_iter': 3000}\n0.653 (+/-0.037) for {'loss': 'log', 'max_iter': 3000}\n0.608 (+/-0.018) for {'loss': 'squared_hinge', 'max_iter': 3000}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       0.63      1.00      0.77     88789\n           1       0.00      0.00      0.00     52713\n\n    accuracy                           0.63    141502\n   macro avg       0.31      0.50      0.39    141502\nweighted avg       0.39      0.63      0.48    141502\n\n\nExecution time: 140.95240998268127\n"
    }
   ],
   "source": [
    "import time\n",
    "starting_tm = time.time()\n",
    "sgd_clf = SGDClassifier(n_jobs=12)\n",
    "msg =\"Using Stohastic Gradient Descent\"\n",
    "tuned_parameters = [{'loss':['hinge', 'modified_huber', 'log', 'squared_hinge'], 'max_iter':[3000]}]\n",
    "scores = ['accuracy']\n",
    "grid_evaluattion(sgd_clf, msg, train, y, tuned_parameters, scores)\n",
    "print(\"Execution time: \" + str(time.time() - starting_tm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Using Random Forest Classifier\n# Tuning hyper-parameters for accuracy\n\nBest parameters set found on development set: \n{'n_estimators': 460}\n\nGrid scores on development set:\n0.742 (+/-0.001) for {'n_estimators': 300}\n0.741 (+/-0.002) for {'n_estimators': 310}\n0.742 (+/-0.001) for {'n_estimators': 320}\n0.742 (+/-0.001) for {'n_estimators': 330}\n0.742 (+/-0.002) for {'n_estimators': 340}\n0.742 (+/-0.002) for {'n_estimators': 350}\n0.742 (+/-0.003) for {'n_estimators': 360}\n0.741 (+/-0.002) for {'n_estimators': 370}\n0.742 (+/-0.002) for {'n_estimators': 380}\n0.742 (+/-0.001) for {'n_estimators': 390}\n0.742 (+/-0.001) for {'n_estimators': 400}\n0.742 (+/-0.002) for {'n_estimators': 410}\n0.742 (+/-0.002) for {'n_estimators': 420}\n0.742 (+/-0.000) for {'n_estimators': 430}\n0.742 (+/-0.002) for {'n_estimators': 440}\n0.743 (+/-0.002) for {'n_estimators': 450}\n0.743 (+/-0.002) for {'n_estimators': 460}\n0.742 (+/-0.001) for {'n_estimators': 470}\n0.742 (+/-0.001) for {'n_estimators': 480}\n0.742 (+/-0.001) for {'n_estimators': 490}\n0.743 (+/-0.000) for {'n_estimators': 500}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       0.81      0.77      0.79     88789\n           1       0.64      0.69      0.66     52713\n\n    accuracy                           0.74    141502\n   macro avg       0.72      0.73      0.73    141502\nweighted avg       0.75      0.74      0.74    141502\n\n\n"
    }
   ],
   "source": [
    "## hyper param for Random Forest\n",
    "\n",
    "starting_tm = time.time()\n",
    "rf = RandomForestClassifier(n_jobs=4)\n",
    "msg = \"Using Random Forest Classifier\"\n",
    "tuned_parameters = [{'n_estimators': [i*10 for i in range(30, 51)]}]\n",
    "scores = ['accuracy']\n",
    "grid_evaluattion(rf, msg, train, y, tuned_parameters, scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Classifiers Performance using 283004 documents.\n\nRandom Forest Classifier\nPrecision: 0.7468693052401548\nRecall: 0.7468693052401548\nF1-Measure: 0.7468693052401549\nAccuracy: 0.7468693052401548\nExecution time: 472.4215581417084\n\n\n\nXGBoost\nPrecision: 0.7186294166683436\nRecall: 0.7186294166683436\nF1-Measure: 0.7186294166683435\nAccuracy: 0.7186294166683436\nExecution time: 74.0878894329071\n\n\n\nStohastic Gradient Descent\nPrecision: 0.6302844364241136\nRecall: 0.6302844364241136\nF1-Measure: 0.6302844364241136\nAccuracy: 0.6302844364241136\nExecution time: 117.15589690208435\n\n\n\n"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "print(\"Classifiers Performance using \" + str(train.shape[0]) + \" documents.\\n\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=460, n_jobs=4)\n",
    "# svm_clf =svm.SVC(gamma=0.1, C=10, kernel='rbf')\n",
    "xgb_clf = xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "sgd_clf = SGDClassifier(loss='hinge', max_iter=3000, n_jobs=4)\n",
    "\n",
    "clfs = [(rf, \"Random Forest Classifier\"), (xgb_clf, \"XGBoost\"), (sgd_clf, \"Stohastic Gradient Descent\")] #(svm_clf, \"SVM Classifier\")\n",
    "for clf, clf_name in clfs:\n",
    "    evaluation(clf, clf_name, train, y)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Random Forest 430\nPrecision: 0.7464806220874293\nRecall: 0.7464806220874293\nF1-Measure: 0.7464806220874293\nAccuracy: 0.7464806220874293\nExecution time: 499.32548666000366\n"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=460, n_jobs=4)\n",
    "evaluation(rf, \"Random Forest 460\", train, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test_without_labels.csv\")\n",
    "\n",
    "test['len_q1'] = test[\"Question1\"].apply(lambda x: len(str(x)))\n",
    "test['len_q2'] = test[\"Question2\"].apply(lambda x: len(str(x)))\n",
    "test['diff_len'] = test.len_q1 - test.len_q2\n",
    "test['len_char_q1'] = test[\"Question1\"].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "test['len_char_q2'] = test[\"Question2\"].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "test['len_word_q1'] = test[\"Question1\"].apply(lambda x: len(str(x).split()))\n",
    "test['len_word_q2'] = test[\"Question2\"].apply(lambda x: len(str(x).split()))\n",
    "test['common_words'] = test.apply(lambda x: len(set(str(x[[\"Question1\"]]).lower().split()).intersection(set(str(x['Question2']).lower().split()))), axis=1)\n",
    "\n",
    "test['fuzz_qratio'] = test.apply(lambda x: fuzz.QRatio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "test['fuzz_WRatio'] = test.apply(lambda x: fuzz.WRatio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "test['fuzz_partial_ratio'] = test.apply(lambda x: fuzz.partial_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "test['fuzz_partial_token_set_ratio'] = test.apply(lambda x:fuzz.partial_token_set_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "test['fuzz_partial_token_sort_ratio'] = test.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "test['fuzz_token_set_ratio'] = test.apply(lambda x: fuzz.token_set_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "test['fuzz_token_sort_ratio'] = test.apply(lambda x: fuzz.token_sort_ratio(str(x['Question1']), str(x['Question2'])), axis=1)\n",
    "\n",
    "#---\n",
    "\n",
    "w2v_q1 = np.array([sent2vec(q, model) for q in test[\"Question1\"]])\n",
    "w2v_q2 = np.array([sent2vec(q, model) for q in test[\"Question2\"]])\n",
    "\n",
    "test['cosine_distance'] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "test['cityblock_distance'] = [cityblock(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "test['jaccard_distance'] = [jaccard(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "test['canberra_distance'] = [canberra(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "test['euclidean_distance'] = [euclidean(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "test['minkowski_distance'] = [minkowski(x,y,3) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "test['braycurtis_distance'] = [braycurtis(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = test[ feats_1 + feats_2 + feats_3 ]\n",
    "dtest = dtest.fillna(dtest.mean())\n",
    "vtest = dtest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_store(d, \"vars/testf1f2f3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediciting with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=460, n_jobs=4)\n",
    "rf.fit(train, y)\n",
    "predictions = rf.predict(vtest)\n",
    "\n",
    "predictions_df = pd.DataFrame(data={'Id': list(test['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(\"data/predictionsRFfull460.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "xgb_clf.fit(train, y)\n",
    "predictions = xgb_clf.predict(vtest)\n",
    "\n",
    "predictions_df = pd.DataFrame(data={'Id': list(test['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(\"data/predictionsXGBfull.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(data={'Id': list(test['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(\"data/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}