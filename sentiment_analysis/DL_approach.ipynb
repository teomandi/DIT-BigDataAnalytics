{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "semantic_analysis.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyPn3jEMATAUbiCgzOv7OdIu"
  },
  "kernelspec": {
   "name": "pycharm-80ab35c8",
   "language": "python",
   "display_name": "PyCharm (DSIT-BigDataAnalytics)"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6vhie7hv-5T",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis using DL techniques\n",
    "In this notebook we try to classify IMDB movie reviews as happy or not using DL tecniques. In more details,\n",
    "we use word2vec to train our word2vec model and transform words into word embeddings. Then we train and evaluate an RNN\n",
    "model. In the end, we use it to predict the sentiment of actual IMDB reviews accomplishing **Accuracy up to 0.8713**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "train_path = \"files/data/train.csv\"\n",
    "predicitions_path = 'files/data/predictions_dl.csv'\n",
    "test_path = \"files/data/test_without_labels.csv\"\n",
    "embeddings_path = \"files/data/embeddings.txt\"\n",
    "\n",
    "embedding_dim = 100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6OQnEfmwEz0",
    "colab_type": "code",
    "outputId": "6f4b3d0c-53b5-4965-89fb-749c3efeaafa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1582047009532,
     "user_tz": -120,
     "elapsed": 1022,
     "user": {
      "displayName": "George Mandilaras",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCzd5HI4gAjU5MVobIZ28PwljJ1VbxCQs2GkhXIaQ=s64",
      "userId": "05355328814917761133"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Colab Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_path = '/content/drive/My Drive/Colab Notebooks/BDA/semantic_analysis/train.csv'\n",
    "test_path = '/content/drive/My Drive/Colab Notebooks/BDA/semantic_analysis/test_without_labels.csv'\n",
    "predicitions_path = '/content/drive/My Drive/Colab Notebooks/BDA/semantic_analysis/predictions_dl.csv'\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi6nNUeZwICK",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "\n",
    "X = train['Content']\n",
    "y = train['Label']\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "X_test = test['Content']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-zmIkA3wMSe",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleaning Text\n",
    "In this method we tokenize the reviews and clean them from punctuations and non-alpha characters\n",
    "but we don't remove stopwords, as they sometimes are considered useful in Sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "punctuation_table = str.maketrans('','', string.punctuation)\n",
    "\n",
    "def clean_text(reviews):\n",
    "  clean_reviews = list()\n",
    "  lines = reviews.values.tolist()\n",
    "\n",
    "  for line in lines:\n",
    "      tokens = []\n",
    "      for token in word_tokenize(line):\n",
    "          token = token.lower()\n",
    "          stripped = token.translate(punctuation_table)\n",
    "          if token.isalpha() :\n",
    "              tokens.append(stripped)\n",
    "      clean_reviews.append(tokens)\n",
    "      \n",
    "  return clean_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msmN23Sx5Te_",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "2fbd622e-bf19-43a8-f5d5-22881f821bc1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1582041001661,
     "user_tz": -120,
     "elapsed": 66767,
     "user": {
      "displayName": "George Mandilaras",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCzd5HI4gAjU5MVobIZ28PwljJ1VbxCQs2GkhXIaQ=s64",
      "userId": "05355328814917761133"
     }
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess and Embeddings Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import  pad_sequences\n",
    "\n",
    "review_lines = clean_text(X)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_lines)\n",
    "review_seq = tokenizer.texts_to_sequences(review_lines)\n",
    "\n",
    "max_length = max([len(review.split()) for review in X])\n",
    "review_pad = pad_sequences(review_seq, max_length)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Unique Tokens \" + str(vocab_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMRbhxmRwQdt",
    "colab_type": "code",
    "outputId": "2519284f-1138-404c-c4e7-418f1e0de472",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1582041151277,
     "user_tz": -120,
     "elapsed": 36919,
     "user": {
      "displayName": "George Mandilaras",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCzd5HI4gAjU5MVobIZ28PwljJ1VbxCQs2GkhXIaQ=s64",
      "userId": "05355328814917761133"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating our own Word2Vec model based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "gensim_model = gensim.models.Word2Vec(review_lines, size=embedding_dim, window=5, workers=12, min_count=2)\n",
    "\n",
    "words = list(gensim_model.wv.vocab)\n",
    "print(\"Vocabulary size: \" + str(len(words)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuUSL_mewYKE",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Constructing Embedding Matrix\n",
    "The embedding matrix is used by the embedding layer to transform input words to their vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i <= vocab_size and word in gensim_model.wv:\n",
    "        embedding_vector = gensim_model.wv[word]\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeW70QDP6pkf",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## RNN\n",
    "Our model architecture consists of:\n",
    "- An Embedding Layer (not Trainable)\n",
    "- An GRU layer with recurrent drop out rate set to 0.2\n",
    "- A hidden dense layer with 16 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, LSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "def create_model(summary=False):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, \n",
    "                  embeddings_initializer=Constant(embeddings_matrix), \n",
    "                  input_length=max_length, trainable=False),\n",
    "        GRU(units=16, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if summary: model.summary()\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvxedxW_6tX3",
    "colab_type": "code",
    "outputId": "306523cf-2996-4502-c187-a1e2f5d122aa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1582034198600,
     "user_tz": -120,
     "elapsed": 9445930,
     "user": {
      "displayName": "George Mandilaras",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCzd5HI4gAjU5MVobIZ28PwljJ1VbxCQs2GkhXIaQ=s64",
      "userId": "05355328814917761133"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation\n",
    "Perform evaluation using k-Fold Cross Validation. In each iteration the model is constructed\n",
    " from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "starting_tm = time.time()\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "accuracy = 0\n",
    "\n",
    "k = 5\n",
    "epochs = 5\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "for train_index, test_index in skf.split(review_pad, y):\n",
    "    \n",
    "    X_train, X_test = review_pad[train_index], review_pad[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=256)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    predictions = [1 if p[0] > 0.45 else 0 for p in predictions]\n",
    "    \n",
    "    precision += metrics.precision_score(y_test, predictions, average='micro')\n",
    "    recall += metrics.recall_score(y_test, predictions, average='micro')\n",
    "    f1 += metrics.f1_score(y_test, predictions, average='micro')\n",
    "    accuracy += metrics.accuracy_score(y_test, predictions)\n",
    "\n",
    "    print()\n",
    "\n",
    " # compute the average of each value\n",
    "precision_score = precision/k\n",
    "recall_score = recall/k\n",
    "f1_score = f1/k\n",
    "accuracy_score = accuracy/k\n",
    "\n",
    "print(\"Precision: \" + str(precision_score)\n",
    "      + \"\\nRecall: \" + str(recall_score)\n",
    "      + \"\\nF1-Measure: \" + str(f1_score) \n",
    "      + \"\\nAccuracy: \" + str(accuracy_score)\n",
    "      + \"\\nExecution time: \" + str(time.time() - starting_tm))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClD1q8VuAL_P",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prediction\n",
    "Load the testing dataset, pre-process it and then predict it. In the end store the results as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_review_lines = clean_text(X_test)\n",
    "\n",
    "test_review_seq = tokenizer.texts_to_sequences(test_review_lines)\n",
    "test_review_pad = pad_sequences(test_review_seq, max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_model(True)\n",
    "model.fit(review_pad, y, epochs=10, batch_size=256)\n",
    "predictions = model.predict(test_review_pad)\n",
    "predictions = [1 if p[0] > 0.45 else 0 for p in predictions]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "predictions_df = pd.DataFrame(data={'Id': list(test['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(predicitions_path, index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}