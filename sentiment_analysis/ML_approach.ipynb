{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis using ML techniques\n",
    "In this notebook we try to classify IMDB movie reviews as happy or not using ML. In more details\n",
    "we clean the reviews and then vectorize them using TF-IDF weight and then we use Baggings SVMs for classification.\n",
    "We evaluate our classifier using K-Fold Cross Validation, and then use it to predict the sentiment of\n",
    "actual IMDB reviews. \n",
    "### In the end accomplishes Accuracy up to  0.86200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "train_path = \"files/data/train.csv\"\n",
    "test_path = \"files/data/test_without_labels.csv\"\n",
    "x_vectors_path = \"files/serialized/vectors\"\n",
    "test_vectors_path = \"files/serialized/tets_vectors\"\n",
    "predictions_path = \"files/data/predictions.csv\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pickle Store and load\n",
    "Used in order to store and load the produced vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def pickle_store(obj, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def pickle_load(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Data from Local store"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "\n",
    "X = train['Content']\n",
    "y = train['Label']\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "X_test = test['Content']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process using Lemmatization\n",
    "\n",
    "Applying Lemmatization using position tags. We use position tags in order to enable lemmatization, \n",
    "not only to nouns but also to all other parts of speech. Also removing stopwords, punctuations and non alpha characters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:    \n",
    "      return None\n",
    "\n",
    "def documents_preprocess(documents):\n",
    "    new_documents = []\n",
    "    starting_tm = time.time()\n",
    "    for doc in documents:\n",
    "        clean_doc = []\n",
    "        doc_tokens = simple_preprocess(doc, deacc=True)\n",
    "        for word, nltk_tag in  nltk.pos_tag(doc_tokens):\n",
    "            tag = nltk2wn_tag(nltk_tag)\n",
    "            if tag is not None:\n",
    "                clean_doc.append(lmtzr.lemmatize(word, tag))\n",
    "            else:\n",
    "                clean_doc.append(word)\n",
    "        new_documents.append(clean_doc)\n",
    "    \n",
    "    print(\"Text Preprocessing took: \" + str(time.time() - starting_tm))\n",
    "    return new_documents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text pre-process and Vectorization\n",
    "We use the hashing trick and then tf-idf transformer in order to convert words \n",
    "frequencies into TF-IDF values. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=100000, lowercase=False, tokenizer=lambda x: x)\n",
    "\n",
    "def tfidf_vectorization(documents):\n",
    "    starting_tm = time.time()\n",
    "    vectors = vectorizer.fit_transform(documents)\n",
    "    vectors = TfidfTransformer().fit_transform(vectors)\n",
    "    print(\"Vectorization time: \" + str((time.time() - starting_tm)))\n",
    "    return vectors\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation using 5-Fold Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluation(clf, clf_name, X, y, k=5):\n",
    "    starting_tm = time.time()\n",
    "    clf_precision = 0\n",
    "    clf_recall = 0\n",
    "    clf_f1 = 0\n",
    "    clf_accuracy = 0\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        clf_precision += metrics.precision_score(y_test, predictions, average='micro')\n",
    "        clf_recall += metrics.recall_score(y_test, predictions, average='micro')\n",
    "        clf_f1 += metrics.f1_score(y_test, predictions, average='micro')\n",
    "        clf_accuracy += metrics.accuracy_score(y_test, predictions)\n",
    "    \n",
    "     # compute the average of each value\n",
    "    precision_score = clf_precision/k\n",
    "    recall_score = clf_recall/k\n",
    "    f1_score = clf_f1/k\n",
    "    accuracy_score = clf_accuracy/k\n",
    "    \n",
    "    print(clf_name + \"\\nPrecision: \" + str(precision_score)\n",
    "          + \"\\nRecall: \" + str(recall_score)\n",
    "          + \"\\nF1-Measure: \" + str(f1_score) \n",
    "          + \"\\nAccuracy: \" + str(accuracy_score)\n",
    "          + \"\\nExecution time: \" + str(time.time() - starting_tm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process and vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = documents_preprocess(X)\n",
    "X_vectors = tfidf_vectorization(X)\n",
    "\n",
    "pickle_store(X_vectors, x_vectors_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "X_vectors = pickle_load(x_vectors_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "n = 8\n",
    "svm_clf = svm.SVC(gamma=0.1, C=10, kernel='rbf')\n",
    "bagging_clf = BaggingClassifier(svm_clf, n_estimators=n, max_samples=1/n, n_jobs=n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation(bagging_clf, \"Baggings SVM\",  X_vectors, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing set Pre-process and Vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = documents_preprocess(X_test)\n",
    "X_test_vectors = tfidf_vectorization(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting and storing the results as CSV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bagging_clf.fit(X_vectors, y)\n",
    "predictions = bagging_clf.predict(X_test_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(data={'Id': list(test['Id']), 'Predicted':predictions})\n",
    "predictions_df.to_csv(predictions_path, index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}